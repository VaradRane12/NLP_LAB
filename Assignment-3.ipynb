{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2c129baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "string = \"I'm a Student in Pccoe. I'm in the branch Information Technology. I like Running, and dancing, and playing games\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "65309100",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c2d5c62c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"punkt_tab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6293aebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = string.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6bd40626",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"i'm a student in pccoe. i'm in the branch information technology. i like running, and dancing, and playing games\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8ac78191",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = nltk.word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "13251a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokens = nltk.word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "65a9106c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " \"'m\",\n",
       " 'a',\n",
       " 'student',\n",
       " 'in',\n",
       " 'pccoe',\n",
       " '.',\n",
       " 'i',\n",
       " \"'m\",\n",
       " 'in',\n",
       " 'the',\n",
       " 'branch',\n",
       " 'information',\n",
       " 'technology',\n",
       " '.',\n",
       " 'i',\n",
       " 'like',\n",
       " 'running',\n",
       " ',',\n",
       " 'and',\n",
       " 'dancing',\n",
       " ',',\n",
       " 'and',\n",
       " 'playing',\n",
       " 'games']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3a5e8ab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "lemma = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8886b78d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'playing'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma.lemmatize(\"playing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5adc9fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n,i in enumerate(tokens):\n",
    "    tokens[n] = lemma.lemmatize(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "708ba4d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'m\", 'student', 'pccoe', '.', \"'m\", 'branch', 'information', 'technology', '.', 'like', 'running', ',', 'dancing', ',', 'playing', 'game']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "sw = set(stopwords.words(\"english\"))\n",
    "\n",
    "filtered_tokens = [t for t in tokens if t.lower() not in sw]\n",
    "\n",
    "print(filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e2a1f417",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"'m\",\n",
       " 'student',\n",
       " 'pccoe',\n",
       " '.',\n",
       " \"'m\",\n",
       " 'branch',\n",
       " 'information',\n",
       " 'technology',\n",
       " '.',\n",
       " 'like',\n",
       " 'running',\n",
       " ',',\n",
       " 'dancing',\n",
       " ',',\n",
       " 'playing',\n",
       " 'game']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f1337dfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d979a491",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "clean_tokens = [t for t in filtered_tokens if t not in string.punctuation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "108e8436",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"'m\",\n",
       " 'student',\n",
       " 'pccoe',\n",
       " \"'m\",\n",
       " 'branch',\n",
       " 'information',\n",
       " 'technology',\n",
       " 'like',\n",
       " 'running',\n",
       " 'dancing',\n",
       " 'playing',\n",
       " 'game']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "093c61d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748d9f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = list(ngrams(clean_tokens,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dc4da04f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('i', \"'m\"),\n",
       " (\"'m\", 'a'),\n",
       " ('a', 'student'),\n",
       " ('student', 'in'),\n",
       " ('in', 'pccoe'),\n",
       " ('pccoe', '.'),\n",
       " ('.', 'i'),\n",
       " ('i', \"'m\"),\n",
       " (\"'m\", 'in'),\n",
       " ('in', 'the'),\n",
       " ('the', 'branch'),\n",
       " ('branch', 'information'),\n",
       " ('information', 'technology'),\n",
       " ('technology', '.'),\n",
       " ('.', 'i'),\n",
       " ('i', 'like'),\n",
       " ('like', 'running'),\n",
       " ('running', ','),\n",
       " (',', 'and'),\n",
       " ('and', 'dancing'),\n",
       " ('dancing', ','),\n",
       " (',', 'and'),\n",
       " ('and', 'playing'),\n",
       " ('playing', 'game')]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5842aed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngrams(n, tokens):\n",
    "    ngrams = []\n",
    "    tokens = (n-1)*['<START>']+tokens\n",
    "    print(\"TOKENS: \",tokens)\n",
    "    for i in range(n - 1, len(tokens)):\n",
    "        context = tuple(print(i - p - 1, p) for p in reversed(range(n - 1)))\n",
    "        target_word = tokens[i]\n",
    "        ngrams.append((context, target_word))\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7039b8dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "range(0, 2)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "range(3-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c6293c6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "for i in reversed(range(3-1)):\n",
    "    print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "921165dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOKENS:  ['<START>', '<START>', '<START>', \"'m\", 'student', 'pccoe', \"'m\", 'branch', 'information', 'technology', 'like', 'running', 'dancing', 'playing', 'game']\n",
      "0 2\n",
      "1 1\n",
      "2 0\n",
      "1 2\n",
      "2 1\n",
      "3 0\n",
      "2 2\n",
      "3 1\n",
      "4 0\n",
      "3 2\n",
      "4 1\n",
      "5 0\n",
      "4 2\n",
      "5 1\n",
      "6 0\n",
      "5 2\n",
      "6 1\n",
      "7 0\n",
      "6 2\n",
      "7 1\n",
      "8 0\n",
      "7 2\n",
      "8 1\n",
      "9 0\n",
      "8 2\n",
      "9 1\n",
      "10 0\n",
      "9 2\n",
      "10 1\n",
      "11 0\n",
      "10 2\n",
      "11 1\n",
      "12 0\n",
      "11 2\n",
      "12 1\n",
      "13 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[((None, None, None), \"'m\"),\n",
       " ((None, None, None), 'student'),\n",
       " ((None, None, None), 'pccoe'),\n",
       " ((None, None, None), \"'m\"),\n",
       " ((None, None, None), 'branch'),\n",
       " ((None, None, None), 'information'),\n",
       " ((None, None, None), 'technology'),\n",
       " ((None, None, None), 'like'),\n",
       " ((None, None, None), 'running'),\n",
       " ((None, None, None), 'dancing'),\n",
       " ((None, None, None), 'playing'),\n",
       " ((None, None, None), 'game')]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_ngrams(4,clean_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4263d10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is a sample text for n gram language model . the model will learn from this\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from nltk.corpus import brown\n",
    "words = list(brown.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "18455e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'I am planning to____________________________________'\n",
    "start_sentence = 'I am planning to'\n",
    "tokens = start_sentence.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "84988fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fourgram_model(self):\n",
    "    next_words = []\n",
    "    for i in range(len(words)-3):\n",
    "                if words[i] == tokens[-3]:\n",
    "                    if words[i+1] == tokens[-2]:\n",
    "                        if words[i+2] == tokens[-1]:\n",
    "                            next_words.append(words[i+3])\n",
    "    next_words = next_words\n",
    "    return next_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c1d62b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_3_next_words(next_words):\n",
    "    next_words_dict = dict()\n",
    "    for word in next_words:\n",
    "        if not word in next_words_dict.keys():\n",
    "            next_words_dict[word] = 1\n",
    "        else:\n",
    "            next_words_dict[word] += 1\n",
    "    for i,j in next_words_dict.items():\n",
    "                next_words_dict[i] = np.round(j/len(next_words),2)\n",
    "    #Sorting the probs in decreasing order and select the first three\n",
    "    result = sorted(next_words_dict.items(), key = lambda k:(k[1], k[0]), reverse=True)[:3]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f44c5673",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\Asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i am planning to expand its eradication program soon to four additional counties -- troup , pierce , bryan and bulloch -- to treat 132,000 acres infested by the ants , according to an\n"
     ]
    }
   ],
   "source": [
    "# FINAL: N-gram text generation with backoff (4-gram → 3-gram → 2-gram → 1-gram)\n",
    "# Matches the Analytics Vidhya article logic exactly\n",
    "# Uses Brown corpus from NLTK\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "\n",
    "nltk.download(\"brown\")\n",
    "\n",
    "words = [w.lower() for w in brown.words()]\n",
    "\n",
    "\n",
    "def fourgram_next(tokens):\n",
    "    out = []\n",
    "    for i in range(len(words) - 3):\n",
    "        if words[i:i+3] == tokens[-3:]:\n",
    "            out.append(words[i+3])\n",
    "    return out\n",
    "\n",
    "def trigram_next(tokens):\n",
    "    out = []\n",
    "    for i in range(len(words) - 2):\n",
    "        if words[i:i+2] == tokens[-2:]:\n",
    "            out.append(words[i+2])\n",
    "    return out\n",
    "\n",
    "def bigram_next(tokens):\n",
    "    out = []\n",
    "    for i in range(len(words) - 1):\n",
    "        if words[i] == tokens[-1]:\n",
    "            out.append(words[i+1])\n",
    "    return out\n",
    "\n",
    "def unigram_next():\n",
    "    return [random.choice(words)]\n",
    "\n",
    "\n",
    "def top_k(words_list, k=3):\n",
    "    if not words_list:\n",
    "        return []\n",
    "    freq = {}\n",
    "    for w in words_list:\n",
    "        freq[w] = freq.get(w, 0) + 1\n",
    "    total = len(words_list)\n",
    "    probs = {w: round(c / total, 3) for w, c in freq.items()}\n",
    "    return sorted(probs.items(), key=lambda x: (-x[1], x[0]))[:k]\n",
    "\n",
    "\n",
    "def generate_text(start_sentence, n_words=30):\n",
    "    tokens = start_sentence.lower().split()\n",
    "\n",
    "    for _ in range(n_words):\n",
    "        candidates = fourgram_next(tokens)\n",
    "        if not candidates:\n",
    "            candidates = trigram_next(tokens)\n",
    "        if not candidates:\n",
    "            candidates = bigram_next(tokens)\n",
    "        if not candidates:\n",
    "            candidates = unigram_next()\n",
    "\n",
    "        top = top_k(candidates)\n",
    "        if not top:\n",
    "            break\n",
    "\n",
    "        next_word = random.choice(top)[0]\n",
    "        tokens.append(next_word)\n",
    "\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "start = \"I am planning to\"\n",
    "generated = generate_text(start, 30)\n",
    "print(generated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b28c4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
